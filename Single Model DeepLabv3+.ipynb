{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49873d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 1024, 1024\n",
    "NUM_CLASSES = 3 # 0. Background, 1, Exudatas(Hard + Soft), 2. Red Dot + Hemorrhages + Microaneurysms\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "# On Colab\n",
    "MODEL_DIR = \"./Diabetic Retinopathy/\"\n",
    "BASE_DIR = './DR_data/'\n",
    "\n",
    "palette = [[0],[100],[200]]#100：(green)Red Dot + Hemorrhages + Microaneurysms 200：(yellow)Exudatas(Hard + Soft)\n",
    "category_types = [\"Background\", \"EX\", \"RHM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a122acd",
   "metadata": {},
   "source": [
    "# Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import metrics\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# from keras.layers.Layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input\n",
    "# from keras.keras.layers import DepthwiseConv2D, ZeroPadding2D, AveragePooling2D, Concatenate, Dropout, Conv2DTranspose\n",
    "# from keras import layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "# from keras.layers.merge import concatenate\n",
    "from PIL import Image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4439d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASPP(tensor):\n",
    "    '''atrous spatial pyramid pooling'''\n",
    "    dims = K.int_shape(tensor)\n",
    "\n",
    "    y_pool = keras.layers.AveragePooling2D(pool_size=(dims[1], dims[2]), name='average_pooling')(tensor)\n",
    "    y_pool = keras.layers.Conv2D(filters=512, kernel_size=1, padding='same',\n",
    "                    kernel_initializer='he_normal', name='pool_1x1conv2d', use_bias=False)(y_pool)\n",
    "#     print(y_pool.shape)# (None, 1, 1, 256)\n",
    "    y_pool = keras.layers.BatchNormalization(name=f'bn_1')(y_pool)\n",
    "    y_pool = keras.layers.Activation('relu', name=f'relu_1')(y_pool)\n",
    "\n",
    "    # y_pool = Upsample(tensor=y_pool, size=[dims[1], dims[2]])\n",
    "    y_pool = keras.layers.Conv2DTranspose(filters=512, kernel_size=(2, 2), \n",
    "                  kernel_initializer='he_normal', dilation_rate=1024 // 16 - 1)(y_pool)\n",
    "\n",
    "    y_1 = keras.layers.Conv2D(filters=512, kernel_size=1, dilation_rate=1, padding='same',\n",
    "                 kernel_initializer='he_normal', name='ASPP_conv2d_d1', use_bias=False)(tensor)\n",
    "    y_1 = keras.layers.BatchNormalization(name=f'bn_2')(y_1)\n",
    "    y_1 = keras.layers.Activation('relu', name=f'relu_2')(y_1)\n",
    "\n",
    "    y_6 = keras.layers.Conv2D(filters=512, kernel_size=3, dilation_rate=6, padding='same',\n",
    "                 kernel_initializer='he_normal', name='ASPP_conv2d_d6', use_bias=False)(tensor)\n",
    "    y_6 = keras.layers.BatchNormalization(name=f'bn_3')(y_6)\n",
    "    y_6 = keras.layers.Activation('relu', name=f'relu_3')(y_6)\n",
    "\n",
    "    y_12 = keras.layers.Conv2D(filters=512, kernel_size=3, dilation_rate=12, padding='same',\n",
    "                  kernel_initializer='he_normal', name='ASPP_conv2d_d12', use_bias=False)(tensor)\n",
    "    y_12 = keras.layers.BatchNormalization(name=f'bn_4')(y_12)\n",
    "    y_12 = keras.layers.Activation('relu', name=f'relu_4')(y_12)\n",
    "\n",
    "    y_18 = keras.layers.Conv2D(filters=512, kernel_size=3, dilation_rate=18, padding='same',\n",
    "                  kernel_initializer='he_normal', name='ASPP_conv2d_d18', use_bias=False)(tensor)\n",
    "    y_18 = keras.layers.BatchNormalization(name=f'bn_5')(y_18)\n",
    "    y_18 = keras.layers.Activation('relu', name=f'relu_5')(y_18)\n",
    "\n",
    "    y = keras.layers.concatenate([y_pool, y_1, y_6, y_12, y_18], name='ASPP_concat')\n",
    "\n",
    "    y = keras.layers.Conv2D(filters=512, kernel_size=1, dilation_rate=1, padding='same',\n",
    "               kernel_initializer='he_normal', name='ASPP_conv2d_final', use_bias=False)(y)\n",
    "    y = keras.layers.BatchNormalization(name=f'bn_final')(y)\n",
    "    y = keras.layers.Activation('relu', name=f'relu_final')(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def DeepLabV3Plus(img_height=1024, img_width=1024, nclasses=3):\n",
    "#     print('*** Building DeepLabv3Plus Network ***')\n",
    "\n",
    "    base_model = keras.applications.ResNet50(input_shape=(img_height, img_width, 3), weights='imagenet', include_top=False)\n",
    "    \n",
    "    image_features = base_model.get_layer('conv4_block6_out').output\n",
    "    x_a = ASPP(image_features)\n",
    "    # x_a = Upsample(tensor=x_a, size=[img_height // 4, img_width // 4])\n",
    "    x_a = keras.layers.Conv2DTranspose(filters=512, kernel_size=(2, 2), \n",
    "                          kernel_initializer='he_normal', dilation_rate=(img_height // 16 * 3))(x_a)\n",
    "#     print('hhhhhhhhhhhhhhhhhhh',x_a.shape)# (None, 128, 128, 256)\n",
    "\n",
    "    x_b = base_model.get_layer('conv2_block3_out').output\n",
    "    x_b = keras.layers.Conv2D(filters=96, kernel_size=1, padding='same',\n",
    "                 kernel_initializer='he_normal', name='low_level_projection', use_bias=False)(x_b)\n",
    "#     print('cccccccccccccccc',x_b.shape)# (None, 128, 128, 48)\n",
    "    x_b = keras.layers.BatchNormalization(name=f'bn_low_level_projection')(x_b)\n",
    "    x_b = keras.layers.Activation('relu', name='low_level_activation')(x_b)\n",
    "\n",
    "    x = keras.layers.concatenate([x_a, x_b], name='decoder_concat')\n",
    "\n",
    "    x = keras.layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu',\n",
    "               kernel_initializer='he_normal', name='decoder_conv2d_1', use_bias=False)(x)\n",
    "#     print('gggggggggggggg',x.shape)# (None, 128, 128, 256)\n",
    "    x = keras.layers.BatchNormalization(name=f'bn_decoder_1')(x)\n",
    "    x = keras.layers.Activation('relu', name='activation_decoder_1')(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(filters=512, kernel_size=3, padding='same', activation='relu',\n",
    "               kernel_initializer='he_normal', name='decoder_conv2d_2', use_bias=False)(x)\n",
    "#     print('lllllllllllll',x.shape)# (None, 128, 128, 256)\n",
    "    x = keras.layers.BatchNormalization(name=f'bn_decoder_2')(x)\n",
    "    x = keras.layers.Activation('relu', name='activation_decoder_2')(x)\n",
    "    # x = Upsample(x, [img_height, img_width])\n",
    "    x = keras.layers.Conv2DTranspose(filters=512, kernel_size=(2, 2), \n",
    "                        kernel_initializer='he_normal', dilation_rate=img_height // 4 * 3)(x)\n",
    "#     print(\"nnnnnnnnnnnnnnnnnnnn\",x.shape)# (None, 512, 512, 256)\n",
    "\n",
    "    x = keras.layers.Conv2D(nclasses, (1, 1), name='output_layer')(x)\n",
    "#     print('rrrrrrrrrrrrrrrrr',x.shape)# (None, 512, 512, 1)\n",
    "    x = keras.layers.Activation('softmax')(x) \n",
    "#     print('qqqqqqqqqqqqqqqqqqqq',x.shape)# (None, 512, 512, 1)\n",
    "    '''\n",
    "    x = Activation('softmax')(x) \n",
    "    tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    Args:\n",
    "        from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n",
    "        we assume that `y_pred` encodes a probability distribution.\n",
    "    '''     \n",
    "    model = keras.models.Model(inputs=base_model.input, outputs=x, name='DeepLabV3_Plus')\n",
    "#     print(f'*** Output_Shape => {model.output_shape} ***')\n",
    "    return model\n",
    "# DeepLabV3Plus(nclasses=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59544511",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd324b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dice(y_true, y_pred, solo = True, num_classes = NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Dice\n",
    "    \n",
    "    Dice = 2TP / (2TP + FP + FN) = 2|X∩Y| / (|X| + |Y|) \n",
    "         = sum(2 X*Y) / (sum(X) +sum(Y))\n",
    "    \"\"\" \n",
    "    smooth = 0.0001\n",
    "    if solo:\n",
    "        y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32),\n",
    "                            depth=num_classes,\n",
    "                            dtype=tf.float32,\n",
    "                            )\n",
    "        \n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "    return (numerator + smooth) / (denominator + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(y_true, y_pred, solo = True, num_classes = NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Jaccard\n",
    "    \n",
    "    IoU = TP / (TP + FP + FN) = |X∩Y| / ( |X| + |Y| - |X∩Y| )\n",
    "        = sum(A*B) / (sum(A)+sum(B)-sum(A*B))\n",
    "    \"\"\"\n",
    "    smooth = 0.0001\n",
    "    if solo:\n",
    "        y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32),\n",
    "                            depth=num_classes,\n",
    "                            dtype=tf.float32,\n",
    "                            )\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    sum_ = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
    "    return ((intersection + smooth) / (sum_ - intersection + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalFocalLoss(tf.losses.Loss):\n",
    "    \"\"\"\n",
    "    식 : loss = - y_true * alpha * ((1 - y_pred)^gamma) * log(y_pred)\n",
    "        \n",
    "    alpha: the same as weighting factor in balanced cross entropy, default 0.25\n",
    "    gamma: focusing parameter for modulating factor (1-p), default 2.0\n",
    "\n",
    "    y_true =  [[0., 1.0, 0.], [0., 0., 1.], [0., 1., 0.]]\n",
    "    y_pred = [[0.70, 0.15, 0.15], [0.1, 0.8, 0.1], [0.25, 0.65, 0.1]]\n",
    "    y_true = tf.cast(y_true, dtype= \"float32\")\n",
    "    y_pred = tf.cast(y_pred, dtype= \"float32\")\n",
    "    gamma=3.0\n",
    "    alpha=0.25\n",
    "    \"\"\"\n",
    "    def __init__(self, solo = True, num_classes = NUM_CLASSES, gamma = 2.0, alpha=0.25):\n",
    "        super(CategoricalFocalLoss, self).__init__(reduction = 'auto', name = \"CategoricalFocalLoss\")\n",
    "        self._num_classes = num_classes\n",
    "        self._gamma = gamma\n",
    "        self._alpha = alpha\n",
    "        self._epsilon = 1e-07\n",
    "        self.solo = solo\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.solo:\n",
    "            y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32),\n",
    "                                depth=self._num_classes,\n",
    "                                dtype=tf.float32,\n",
    "                                )\n",
    "#         print(\"y_trueeeeeee\",y_true.shape)\n",
    "#         print(\"y_preddddddd\",y_pred.shape)\n",
    "        y_pred = tf.clip_by_value(y_pred, self._epsilon, 1.0 - self._epsilon)      \n",
    "        loss = - y_true * self._alpha * tf.math.pow((1 - y_pred), self._gamma) * tf.math.log(y_pred)\n",
    "        \"\"\"\n",
    "        Another Code\n",
    "        alpha = tf.where(tf.equal(y_true, 1.), alpha, (1.0 - self._alpha))\n",
    "        pt = tf.where(tf.equal(y_true, 1.), y_pred, 1-y_pred)\n",
    "        y_pred = tf.add(y_pred, self._epsilon)\n",
    "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * tf.multiply(y_true, -tf.math.log(y_pred))\n",
    "        \"\"\"\n",
    "        return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a532678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Dice loss------------\n",
    "def dice_coef_fun(smooth=1,solo = True):\n",
    "    def dice_coef(y_true, y_pred):\n",
    "        if solo:\n",
    "            y_true = tf.one_hot(tf.cast(y_true, dtype=tf.int32),\n",
    "                                depth=NUM_CLASSES,\n",
    "                                dtype=tf.float32,\n",
    "                                )\n",
    "        \n",
    "        intersection = K.sum(y_true * y_pred, axis=(1,2))\n",
    "        union = K.sum(y_true, axis=(1,2)) + K.sum(y_pred, axis=(1,2))\n",
    "        sample_dices=(2. * intersection + smooth) / (union + smooth) \n",
    "        \n",
    "        dices=K.mean(sample_dices,axis=0)\n",
    "        return K.mean(dices)\n",
    "    return dice_coef\n",
    " \n",
    "def dice_coef_loss_fun(smooth=0):\n",
    "    def dice_coef_loss(y_true,y_pred):\n",
    "        return 1-dice_coef_fun(smooth=smooth)(y_true=y_true,y_pred=y_pred)#1-1-?\n",
    "    return dice_coef_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e86b0d",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class Dataset_Generator():\n",
    "    def __init__(self,\n",
    "                 base_dir = BASE_DIR,\n",
    "                 num_classes = NUM_CLASSES,\n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 height = HEIGHT,\n",
    "                 width = WIDTH,\n",
    "                 epochs = EPOCHS,\n",
    "                ):\n",
    "        \n",
    "        self.base_dir = BASE_DIR\n",
    "        self.num_classes = float(num_classes)\n",
    "        self.batch_size = batch_size\n",
    "        self.height = HEIGHT\n",
    "        self.width = WIDTH\n",
    "        self.epochs = epochs\n",
    "        self.class_values = list(range(len(category_types)))\n",
    "        #self.images_list = []\n",
    "        self.images_list = os.listdir(self.base_dir + \"Training/images/\")\n",
    "        random.shuffle(self.images_list)\n",
    "        \n",
    "    def __del__(self):\n",
    "        print(\"Dataset Generator is destructed\")\n",
    "            \n",
    "    def _preprocessor(self):\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(self.base_dir+\"Training\")\n",
    "            os.mkdir(self.base_dir+\"Test\")\n",
    "            os.mkdir(self.base_dir+\"Training/images\")\n",
    "            os.mkdir(self.base_dir+\"Test/images\")\n",
    "            os.mkdir(self.base_dir+\"Training/masks\")\n",
    "            os.mkdir(self.base_dir+\"Test/masks\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        idrid_cnt = diaretdb_cnt = 0 \n",
    "\n",
    "        image_list = os.listdir(self.base_dir + \"image/\")\n",
    "        for i, file_name in enumerate(image_list):\n",
    "            image_list[i] = file_name.split(\".\")[0]\n",
    "        image_list.sort()\n",
    "\n",
    "        mask_class_dir = [\"MA\", \"HE\", \"EX\", \"SE\"]\n",
    "        mask_file_list = []\n",
    "\n",
    "        for cls in mask_class_dir:\n",
    "            mask_file_list.append(os.listdir(self.base_dir + f\"mask/{cls}\"))\n",
    "\n",
    "        zero_1 = np.zeros([2848, 4288], dtype = np.uint8)\n",
    "        zero_2 = np.zeros([1152, 1500], dtype = np.uint8)\n",
    "\n",
    "        loss_cnt = 0\n",
    "\n",
    "        for i, file_name in enumerate(image_list):\n",
    "            if \"IDRiD\" in file_name:\n",
    "                zero = zero_1\n",
    "                thres = 1\n",
    "            elif \"image\" in file_name:\n",
    "                zero = zero_2\n",
    "                # [63, 127, 189, 252]\n",
    "                thres = 127\n",
    "\n",
    "            mask_list = []\n",
    "        \n",
    "            for cls in range(4):\n",
    "              \n",
    "                if \"IDRiD\" in file_name:\n",
    "                    mask_file_name = f\"{file_name}_{mask_class_dir[cls]}.tif\"\n",
    "                elif \"image\" in file_name:\n",
    "                    mask_file_name = f\"{file_name}.png\"\n",
    "\n",
    "             \n",
    "                if mask_file_name in mask_file_list[cls]:\n",
    "                    mask = cv2.imread(f\"{self.base_dir}mask/{mask_class_dir[cls]}/{mask_file_name}\", 0)\n",
    "                    _, mask = cv2.threshold(mask, thres, 1, cv2.THRESH_BINARY)\n",
    "                else:\n",
    "                    mask = zero\n",
    "                mask_list.append(mask)\n",
    "\n",
    "            Class_1 = cv2.bitwise_or(mask_list[0], mask_list[1]) * 100\n",
    "            Class_2 = cv2.bitwise_or(mask_list[2], mask_list[3]) * 200\n",
    "            mask = Class_1 + Class_2\n",
    "            del Class_1, Class_2, mask_list\n",
    "\n",
    "            if np.all(mask == zero):\n",
    "                loss_cnt += 1\n",
    "                print(f\"{file_name} has no mask\")\n",
    "            else:\n",
    "       \n",
    "                if \"IDRiD\" in file_name:\n",
    "                    file_name = f\"{file_name}.jpg\"\n",
    "                elif \"image\" in file_name:\n",
    "                    file_name = f\"{file_name}.png\"\n",
    "                img = cv2.imread(f\"{self.base_dir}image/{file_name}\")\n",
    "\n",
    "                if \"IDRiD\" in file_name:\n",
    "                    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                    if i==3 or i == 10:\n",
    "                        thres = 10\n",
    "                    else:\n",
    "                        thres = 30\n",
    "\n",
    "                    _, binary_img = cv2.threshold(gray_img, thres, 255, cv2.THRESH_BINARY)\n",
    "                    del gray_img\n",
    "\n",
    "                    # contours\n",
    "                    contours, hierachy = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    \n",
    "                    x_min = np.min(contours[-1], axis = 0)\n",
    "                    x_max = np.max(contours[-1], axis = 0)\n",
    "                    x_min, x_max = x_min[0][0], x_max[0][0]\n",
    "                    del contours, hierachy\n",
    "\n",
    "                    # Crop\n",
    "                    img = img[:, x_min:x_max+1]\n",
    "                    mask = mask[:, x_min:x_max+1]\n",
    "\n",
    "                    # Padding\n",
    "                    if (x_max-x_min)/2848 >= 1.25:\n",
    "                        pad_left, pad_right = 0, 0\n",
    "                    else:\n",
    "                        pad_left, pad_right = 200, 200\n",
    "\n",
    "                    img = cv2.copyMakeBorder(img, 500, 500, pad_left, pad_right, cv2.BORDER_CONSTANT,value=0)\n",
    "                    mask = cv2.copyMakeBorder(mask, 500, 500, pad_left, pad_right, cv2.BORDER_CONSTANT,value=0)\n",
    "\n",
    "                # diaretdb\n",
    "                elif \"image\" in file_name:\n",
    "                    img = cv2.copyMakeBorder(img, 174, 174, 0, 0, cv2.BORDER_CONSTANT,value=0)\n",
    "                    mask = cv2.copyMakeBorder(mask, 174, 174, 0, 0, cv2.BORDER_CONSTANT,value=0)\n",
    "\n",
    "                # Resize\n",
    "                img = cv2.resize(img, dsize=(self.height, self.width), interpolation=cv2.INTER_AREA)\n",
    "                mask = cv2.resize(mask, dsize=(self.height, self.width), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                if \"IDRiD\" in file_name and idrid_cnt < 60:\n",
    "                    cv2.imwrite(f'{self.base_dir}Training/images/{file_name}', img)\n",
    "                    cv2.imwrite(f'{self.base_dir}Training/masks/{file_name}', mask)\n",
    "                    idrid_cnt += 1\n",
    "\n",
    "                elif \"IDRiD\" in file_name and idrid_cnt >= 60:\n",
    "                    cv2.imwrite(f'{self.base_dir}Test/images/{file_name}', img)\n",
    "                    cv2.imwrite(f'{self.base_dir}Test/masks/{file_name}', mask)\n",
    "                    idrid_cnt += 1\n",
    "\n",
    "                elif \"image\" in file_name and diaretdb_cnt < 40:\n",
    "                    cv2.imwrite(f'{self.base_dir}Training/images/{file_name}', img)\n",
    "                    cv2.imwrite(f'{self.base_dir}Training/masks/{file_name}', mask)\n",
    "                    diaretdb_cnt += 1\n",
    "\n",
    "                elif \"image\" in file_name and diaretdb_cnt >= 40:\n",
    "                    cv2.imwrite(f'{self.base_dir}Test/images/{file_name}', img)\n",
    "                    cv2.imwrite(f'{self.base_dir}Test/masks/{file_name}', mask)\n",
    "                    diaretdb_cnt += 1\n",
    "\n",
    "                print(f\"{file_name} completed!\")\n",
    "        self.images_list = os.listdir(self.base_dir + \"Training/images/\")\n",
    "        random.shuffle(self.images_list)\n",
    "        print(f\"Preprocessing completed!. Number of no mask data : {loss_cnt}\")\n",
    "    \n",
    "    def _Image_Reshape(self, image, mask):\n",
    "       \n",
    "        image = np.reshape(image, ((self.batch_size,) + image.shape))\n",
    "        mask = np.reshape(mask, ((self.batch_size,) + mask.shape))\n",
    "\n",
    "        return (image/255, mask/100)\n",
    "    \n",
    "    def train_generator(self, k):\n",
    "        \"\"\"\n",
    "        Training Data Augmentation\n",
    "        \"\"\"\n",
    "        if self.images_list:\n",
    "            pass\n",
    "        else:\n",
    "            self._preprocessor()\n",
    "        x_center, y_center = self.width/2, self.height/2\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for i, file_name in enumerate(self.images_list):\n",
    "                if 20*k-20 <= i < 20*k:\n",
    "                    pass\n",
    "                else:\n",
    "             \n",
    "                    img = cv2.imread(f\"{self.base_dir}Training/images/{file_name}\")\n",
    "                    mask = cv2.imread(f\"{self.base_dir}Training/masks/{file_name}\", 0)\n",
    "                \n",
    "                    yield self._Image_Reshape(img, mask)\n",
    "\n",
    "                    flip_img = cv2.flip(img, 1)\n",
    "                    flip_mask = cv2.flip(mask, 1)\n",
    "                    yield self._Image_Reshape(flip_img, flip_mask)\n",
    "\n",
    "                    for degree in range(90, 360, 90):\n",
    "                        matrix = cv2.getRotationMatrix2D((x_center, y_center), degree, 1)\n",
    "                            \n",
    "                        rot_img = cv2.warpAffine(img, matrix, (self.width, self.height))\n",
    "                        rot_mask = cv2.warpAffine(mask, matrix, (self.width, self.height))\n",
    "                        yield self._Image_Reshape(rot_img, rot_mask)\n",
    "\n",
    "                        # filp \n",
    "                        rot_flip_img = cv2.warpAffine(flip_img, matrix, (self.width, self.height))\n",
    "                        rot_flip_mask = cv2.warpAffine(flip_mask, matrix, (self.width, self.height))\n",
    "                        yield self._Image_Reshape(rot_flip_img, rot_flip_mask)\n",
    "\n",
    "\n",
    "    def valid_generator(self, k):\n",
    "        \"\"\"\n",
    "        Validataion Data Augmentation\n",
    "        \"\"\"\n",
    "        x_center, y_center = self.width/2, self.height/2\n",
    "        for _ in range(self.epochs):\n",
    "            for i, file_name in enumerate(self.images_list):\n",
    "  \n",
    "                    img = cv2.imread(f\"{self.base_dir}Training/images/{file_name}\")\n",
    "                    mask = cv2.imread(f\"{self.base_dir}Training/masks/{file_name}\", 0)\n",
    "                                             \n",
    "                    yield self._Image_Reshape(img, mask)\n",
    "\n",
    "                    flip_img = cv2.flip(img, 1)\n",
    "                    flip_mask = cv2.flip(mask, 1)\n",
    "                    yield self._Image_Reshape(flip_img, flip_mask)\n",
    "\n",
    "                    for degree in range(90, 360, 90):\n",
    "                        matrix = cv2.getRotationMatrix2D((x_center, y_center), degree, 1)\n",
    "          \n",
    "                        rot_img = cv2.warpAffine(img, matrix, (self.width, self.height))\n",
    "                        rot_mask = cv2.warpAffine(mask, matrix, (self.width, self.height))\n",
    "                        yield self._Image_Reshape(rot_img, rot_mask)\n",
    "\n",
    "                        # rot_filp \n",
    "                        rot_flip_img = cv2.warpAffine(flip_img, matrix, (self.width, self.height))\n",
    "                        rot_flip_mask = cv2.warpAffine(flip_mask, matrix, (self.width, self.height))\n",
    "                        yield self._Image_Reshape(rot_flip_img, rot_flip_mask)\n",
    "\n",
    "\n",
    "                \n",
    "    def test_generator(self):\n",
    "        images_list = os.listdir(self.base_dir + \"Test/images/\")\n",
    "        for i, file_name in enumerate(images_list):\n",
    "         \n",
    "            img = cv2.imread(f\"{self.base_dir}Test/images/{file_name}\")\n",
    "            mask = cv2.imread(f\"{self.base_dir}Test/masks/{file_name}\",0)\n",
    "#             print(\"111111\",mask.shape)# (1024,1024)\n",
    "                     \n",
    "                    \n",
    "            yield self._Image_Reshape(img, mask)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f968e",
   "metadata": {},
   "source": [
    "# Training the Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "class MODEL():\n",
    "    def __init__(self,\n",
    "                 model_dir = MODEL_DIR,\n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 width = WIDTH,\n",
    "                 height = HEIGHT,\n",
    "                 k = 0,\n",
    "                ):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.loss_fn = dice_coef_loss_fun()#CategoricalFocalLoss()#SoftDiceLoss(NUM_CLASSES)\n",
    "\n",
    "        self.optimizer =  tf.keras.optimizers.Adam(learning_rate=0.00005)# 0.00005\n",
    "        self.generator = Dataset_Generator()\n",
    "        self.model_dir = MODEL_DIR\n",
    "        self.optimal_k = k\n",
    "\n",
    "        self.test_dataset = tf.data.Dataset.from_generator(\n",
    "                        dataset.test_generator,\n",
    "                        (tf.float32, tf.float32),\n",
    "                        (tf.TensorShape([1, HEIGHT, WIDTH, 3]), tf.TensorShape([1,  HEIGHT, WIDTH])),\n",
    "                        )\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "    \n",
    "    def __del__(self):\n",
    "        print(\"MODEL is destructed\")\n",
    "\n",
    "    def Run_training(self, epochs= EPOCHS):        \n",
    "        print(\"Model Complie....\")        \n",
    "        \n",
    "        # K-fold: k = 5\n",
    "        K = 5\n",
    "        mean_Dice = mean_IoU = 0\n",
    "        DiceIoU_list = []\n",
    "        for k in range(1, K+1):\n",
    "            model = DeepLabV3Plus()# Unet()\n",
    "            model.compile(loss =self.loss_fn,#'mse', 'categorical_crossentropy'\n",
    "                          optimizer = self.optimizer,\n",
    "                          metrics = [Dice, Jaccard]\n",
    "                          )\n",
    "#             model.summary()\n",
    "            callbacks_list = [tf.keras.callbacks.ModelCheckpoint(\n",
    "                                        filepath=os.path.join(\n",
    "                                            f\"{self.model_dir}dlbv3plus-Net_{k}.h5\"),\n",
    "                                        monitor=\"val_loss\",\n",
    "#                                         monitor=\"val_Dice\",\n",
    "                                        mode = \"min\",\n",
    "                                       \n",
    "                                        save_best_only=True,\n",
    "                                        save_weights_only=True,\n",
    "                                        verbose=1,\n",
    "                                        ),\n",
    "                              tf.keras.callbacks.EarlyStopping(\n",
    "                                        monitor = 'val_loss',\n",
    "#                                         monitor=\"val_Dice\",\n",
    "                                        mode = \"min\",\n",
    "                                    \n",
    "                                        min_delta = 0.01,\n",
    "                                        patience = 5,\n",
    "                                        )\n",
    "                              ]\n",
    "            print(f\"{k}th fold Start Training....\")\n",
    "            \n",
    "            #  Obtain the current time\n",
    "            start_time = datetime.datetime.now()\n",
    "            \n",
    "            history = model.fit(self.generator.train_generator(k),\n",
    "                                steps_per_epoch = (K-1) * 20 * 8,\n",
    "                                validation_data = self.generator.valid_generator(k),\n",
    "                                validation_steps = 20 * 8,\n",
    "                                callbacks = callbacks_list,\n",
    "                                epochs = epochs,\n",
    "                                batch_size = self.batch_size,\n",
    "                                shuffle = True,\n",
    "                                )\n",
    "            #  Total training time\n",
    "            end_time = datetime.datetime.now()\n",
    "            log_time = \"Total training time: \" + str((end_time - start_time).seconds / 60) + \"m\"\n",
    "            print(log_time)\n",
    "            with open('TrainTime.txt','w') as f:\n",
    "                f.write(log_time)\n",
    "            \n",
    "            loss = history.history['loss']\n",
    "            val_loss = history.history['val_loss']\n",
    "            dice = history.history[\"Dice\"]\n",
    "            val_dice = history.history[\"val_Dice\"]\n",
    "            iou = history.history[\"Jaccard\"]\n",
    "            val_iou = history.history[\"val_Jaccard\"]\n",
    "            \n",
    "            DiceIoU_list.append( val_dice[-1] + val_iou[-1] )\n",
    "            mean_Dice += val_dice[-1]\n",
    "            mean_IoU += val_iou[-1]\n",
    "\n",
    "            epochs_range = range(len(loss))\n",
    "            \n",
    "            plt.figure(k, figsize=(15, 5))\n",
    "\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(epochs_range, loss, label='Training Loss')\n",
    "            plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.title('Loss')\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(epochs_range, dice, label='Training Dice')\n",
    "            plt.plot(epochs_range, val_dice, label='Validation Dice')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.title('Dice Coefficient')\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(epochs_range, iou, label='Training IoU')\n",
    "            plt.plot(epochs_range, val_iou, label='Validation IoU')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.title('IoU')\n",
    "            plt.show()\n",
    "\n",
    "            input_image = tf.keras.Input(shape=(self.width, self.height, 3), name=\"Image\")\n",
    "            predictions = model(input_image, training = True)\n",
    "            inference_model = tf.keras.Model(inputs=input_image, outputs=predictions)\n",
    "\n",
    "            for i, test in enumerate(self.test_dataset):\n",
    "                img, mask = test\n",
    "                prediction = inference_model.predict(img)\n",
    "                \n",
    "                img = img[0].numpy()\n",
    "                mask = mask[0].numpy()\n",
    "#----------------------------True---------------\n",
    "#                 mask = mask[0]\n",
    "#                 mask = (np.argmax(mask, axis=-1)).astype(np.uint8)\n",
    "#                 mask = cv2.resize(mask, (1024, 1024))\n",
    "\n",
    "                prediction = prediction[0]\n",
    "                prediction = tf.math.argmax(prediction, 2)\n",
    "                prediction = prediction.numpy()\n",
    "                \n",
    "                fig = plt.figure(10, figsize = (20,20))\n",
    "                ax1 = fig.add_subplot(1, 3, 1)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                ax1.imshow(img)\n",
    "                ax1.set_title('Image')\n",
    "                ax1.axis(\"off\")\n",
    "\n",
    "                ax2 = fig.add_subplot(1, 3, 2)\n",
    "                ax2.imshow(mask)\n",
    "                ax2.set_title('Ground Truth Mask')\n",
    "                ax2.axis(\"off\")\n",
    "\n",
    "                ax3 = fig.add_subplot(1, 3, 3)\n",
    "                ax3.imshow(prediction)\n",
    "                ax3.set_title('Prediction')\n",
    "                ax3.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "                if i == 1:\n",
    "                    break\n",
    "            del model\n",
    "        print(\"Training End\\n\\n\")\n",
    "        self.optimal_k = DiceIoU_list.index(max(DiceIoU_list)) + 1\n",
    "        print(f\"K-Fold Cross Validation Result\\nmDice : {mean_Dice*20:.3f}, mIoU : {mean_IoU*20:.3f}, Optimal_K : {self.optimal_k}\\n\\n\")\n",
    "\n",
    "        \n",
    "    def Evaluation(self, num_sample):\n",
    "        input_image = tf.keras.Input(shape=(self.width, self.height, 3), name=\"image\")\n",
    "        model = linknet()\n",
    "        model.load_weights(\n",
    "            f\"{self.model_dir}U-Net_5.h5\")\n",
    "\n",
    "        model.compile(loss = self.loss_fn, \n",
    "                      optimizer = self.optimizer,\n",
    "                      metrics = [Dice, Jaccard]\n",
    "                      )\n",
    "\n",
    "        _, dice, iou = model.evaluate(self.test_dataset, batch_size = self.batch_size, verbose= 1)\n",
    "#         print(\"diceeeee\",dice)\n",
    "        print(f\"\\n\\nDice : {dice*100:.2f}, IoU : {iou*100:.2f}\\n\\n\")\n",
    "        predictions = model(input_image, training=True)\n",
    "        inference_model = tf.keras.Model(inputs=input_image, outputs=predictions)\n",
    "        \n",
    "        print(\"Display predictions\")\n",
    "        for i, test in enumerate(self.test_dataset):\n",
    "            img, mask = test\n",
    "            prediction = inference_model.predict(img)\n",
    "            \n",
    "            img = img[0].numpy()\n",
    "            mask = mask[0].numpy()\n",
    "#------------------------True-----------------------\n",
    "#             mask = mask[0]\n",
    "#             mask = (np.argmax(mask, axis=-1)).astype(np.uint8)\n",
    "#             mask = cv2.resize(mask, (1024, 1024))\n",
    "\n",
    "            prediction = prediction[0]\n",
    "            prediction = tf.math.argmax(prediction, -1)\n",
    "            prediction = prediction.numpy()\n",
    "            \n",
    "            fig = plt.figure(i, figsize = (20,20))\n",
    "            ax1 = fig.add_subplot(1, 3, 1)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title('Image')\n",
    "            ax1.axis(\"off\")\n",
    "\n",
    "            ax2 = fig.add_subplot(1, 3, 2)\n",
    "            ax2.imshow(mask)\n",
    "            ax2.set_title('Ground Truth Mask')\n",
    "            ax2.axis(\"off\")\n",
    "\n",
    "            ax3 = fig.add_subplot(1, 3, 3)\n",
    "            ax3.imshow(prediction)\n",
    "            ax3.set_title('Prediction')\n",
    "            ax3.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "            if i == num_sample:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d30dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True  \n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5  \n",
    "# config.gpu_options.visible_device_list = \"0\" \n",
    "# set_session(tf.compat.v1.Session(config=config))\n",
    "vgg_CFL = MODEL()\n",
    "vgg_CFL.Run_training()\n",
    "vgg_CFL.Evaluation(num_sample = -1)\n",
    "del vgg_CFL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
