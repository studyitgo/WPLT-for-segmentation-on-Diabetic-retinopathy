{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import keras.backend as K\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "WIDTH, HEIGHT = 1024, 1024\n",
    "NUM_CLASSES = 6 # 0. Background, 1, Exudatas(Hard + Soft), 2. Red Dot + Hemorrhages + Microaneurysms\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "# On Colab\n",
    "MODEL_DIR = \"./Diabetic Retinopathy/\"\n",
    "BASE_DIR = './DR_data/'\n",
    "\n",
    "palette = [[0],[100],[200]]#100：(green)Red Dot + Hemorrhages + Microaneurysms 200：(yellow)Exudatas(Hard + Soft)\n",
    "category_types = [\"Background\", \"EX\", \"RHM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33daf418",
   "metadata": {},
   "source": [
    "# Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d621acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchfile  # pip install torchfile\n",
    "\n",
    "#From https://github.com/dalgu90/resnet-18-tensorflow/blob/master/extract_torch_t7.py\n",
    "\n",
    "T7_PATH = './resnet-18.t7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(sess, T7_PATH=T7_PATH):\n",
    "    # Open ResNet-18 torch checkpoint\n",
    "    print('Open ResNet-18 torch checkpoint: %s' % T7_PATH)\n",
    "    o = torchfile.load(T7_PATH)\n",
    "\n",
    "    # Load weights in a brute-force way\n",
    "    print('Load weights in a brute-force way')\n",
    "    conv1_weights = o.modules[0].weight\n",
    "    conv1_bn_gamma = o.modules[1].weight\n",
    "    conv1_bn_beta = o.modules[1].bias\n",
    "    conv1_bn_mean = o.modules[1].running_mean\n",
    "    conv1_bn_var = o.modules[1].running_var\n",
    "\n",
    "    conv2_1_weights_1  = o.modules[4].modules[0].modules[0].modules[0].modules[0].weight\n",
    "    conv2_1_bn_1_gamma = o.modules[4].modules[0].modules[0].modules[0].modules[1].weight\n",
    "    conv2_1_bn_1_beta  = o.modules[4].modules[0].modules[0].modules[0].modules[1].bias\n",
    "    conv2_1_bn_1_mean  = o.modules[4].modules[0].modules[0].modules[0].modules[1].running_mean\n",
    "    conv2_1_bn_1_var   = o.modules[4].modules[0].modules[0].modules[0].modules[1].running_var\n",
    "    conv2_1_weights_2  = o.modules[4].modules[0].modules[0].modules[0].modules[3].weight\n",
    "    conv2_1_bn_2_gamma = o.modules[4].modules[0].modules[0].modules[0].modules[4].weight\n",
    "    conv2_1_bn_2_beta  = o.modules[4].modules[0].modules[0].modules[0].modules[4].bias\n",
    "    conv2_1_bn_2_mean  = o.modules[4].modules[0].modules[0].modules[0].modules[4].running_mean\n",
    "    conv2_1_bn_2_var   = o.modules[4].modules[0].modules[0].modules[0].modules[4].running_var\n",
    "    conv2_2_weights_1  = o.modules[4].modules[1].modules[0].modules[0].modules[0].weight\n",
    "    conv2_2_bn_1_gamma = o.modules[4].modules[1].modules[0].modules[0].modules[1].weight\n",
    "    conv2_2_bn_1_beta  = o.modules[4].modules[1].modules[0].modules[0].modules[1].bias\n",
    "    conv2_2_bn_1_mean  = o.modules[4].modules[1].modules[0].modules[0].modules[1].running_mean\n",
    "    conv2_2_bn_1_var   = o.modules[4].modules[1].modules[0].modules[0].modules[1].running_var\n",
    "    conv2_2_weights_2  = o.modules[4].modules[1].modules[0].modules[0].modules[3].weight\n",
    "    conv2_2_bn_2_gamma = o.modules[4].modules[1].modules[0].modules[0].modules[4].weight\n",
    "    conv2_2_bn_2_beta  = o.modules[4].modules[1].modules[0].modules[0].modules[4].bias\n",
    "    conv2_2_bn_2_mean  = o.modules[4].modules[1].modules[0].modules[0].modules[4].running_mean\n",
    "    conv2_2_bn_2_var   = o.modules[4].modules[1].modules[0].modules[0].modules[4].running_var\n",
    "\n",
    "    conv3_1_weights_skip = o.modules[5].modules[0].modules[0].modules[1].weight\n",
    "    conv3_1_weights_1  = o.modules[5].modules[0].modules[0].modules[0].modules[0].weight\n",
    "    conv3_1_bn_1_gamma = o.modules[5].modules[0].modules[0].modules[0].modules[1].weight\n",
    "    conv3_1_bn_1_beta  = o.modules[5].modules[0].modules[0].modules[0].modules[1].bias\n",
    "    conv3_1_bn_1_mean  = o.modules[5].modules[0].modules[0].modules[0].modules[1].running_mean\n",
    "    conv3_1_bn_1_var   = o.modules[5].modules[0].modules[0].modules[0].modules[1].running_var\n",
    "    conv3_1_weights_2  = o.modules[5].modules[0].modules[0].modules[0].modules[3].weight\n",
    "    conv3_1_bn_2_gamma = o.modules[5].modules[0].modules[0].modules[0].modules[4].weight\n",
    "    conv3_1_bn_2_beta  = o.modules[5].modules[0].modules[0].modules[0].modules[4].bias\n",
    "    conv3_1_bn_2_mean  = o.modules[5].modules[0].modules[0].modules[0].modules[4].running_mean\n",
    "    conv3_1_bn_2_var   = o.modules[5].modules[0].modules[0].modules[0].modules[4].running_var\n",
    "    conv3_2_weights_1  = o.modules[5].modules[1].modules[0].modules[0].modules[0].weight\n",
    "    conv3_2_bn_1_gamma = o.modules[5].modules[1].modules[0].modules[0].modules[1].weight\n",
    "    conv3_2_bn_1_beta  = o.modules[5].modules[1].modules[0].modules[0].modules[1].bias\n",
    "    conv3_2_bn_1_mean  = o.modules[5].modules[1].modules[0].modules[0].modules[1].running_mean\n",
    "    conv3_2_bn_1_var   = o.modules[5].modules[1].modules[0].modules[0].modules[1].running_var\n",
    "    conv3_2_weights_2  = o.modules[5].modules[1].modules[0].modules[0].modules[3].weight\n",
    "    conv3_2_bn_2_gamma = o.modules[5].modules[1].modules[0].modules[0].modules[4].weight\n",
    "    conv3_2_bn_2_beta  = o.modules[5].modules[1].modules[0].modules[0].modules[4].bias\n",
    "    conv3_2_bn_2_mean  = o.modules[5].modules[1].modules[0].modules[0].modules[4].running_mean\n",
    "    conv3_2_bn_2_var   = o.modules[5].modules[1].modules[0].modules[0].modules[4].running_var\n",
    "\n",
    "    conv4_1_weights_skip = o.modules[6].modules[0].modules[0].modules[1].weight\n",
    "    conv4_1_weights_1  = o.modules[6].modules[0].modules[0].modules[0].modules[0].weight\n",
    "    conv4_1_bn_1_gamma = o.modules[6].modules[0].modules[0].modules[0].modules[1].weight\n",
    "    conv4_1_bn_1_beta  = o.modules[6].modules[0].modules[0].modules[0].modules[1].bias\n",
    "    conv4_1_bn_1_mean  = o.modules[6].modules[0].modules[0].modules[0].modules[1].running_mean\n",
    "    conv4_1_bn_1_var   = o.modules[6].modules[0].modules[0].modules[0].modules[1].running_var\n",
    "    conv4_1_weights_2  = o.modules[6].modules[0].modules[0].modules[0].modules[3].weight\n",
    "    conv4_1_bn_2_gamma = o.modules[6].modules[0].modules[0].modules[0].modules[4].weight\n",
    "    conv4_1_bn_2_beta  = o.modules[6].modules[0].modules[0].modules[0].modules[4].bias\n",
    "    conv4_1_bn_2_mean  = o.modules[6].modules[0].modules[0].modules[0].modules[4].running_mean\n",
    "    conv4_1_bn_2_var   = o.modules[6].modules[0].modules[0].modules[0].modules[4].running_var\n",
    "    conv4_2_weights_1  = o.modules[6].modules[1].modules[0].modules[0].modules[0].weight\n",
    "    conv4_2_bn_1_gamma = o.modules[6].modules[1].modules[0].modules[0].modules[1].weight\n",
    "    conv4_2_bn_1_beta  = o.modules[6].modules[1].modules[0].modules[0].modules[1].bias\n",
    "    conv4_2_bn_1_mean  = o.modules[6].modules[1].modules[0].modules[0].modules[1].running_mean\n",
    "    conv4_2_bn_1_var   = o.modules[6].modules[1].modules[0].modules[0].modules[1].running_var\n",
    "    conv4_2_weights_2  = o.modules[6].modules[1].modules[0].modules[0].modules[3].weight\n",
    "    conv4_2_bn_2_gamma = o.modules[6].modules[1].modules[0].modules[0].modules[4].weight\n",
    "    conv4_2_bn_2_beta  = o.modules[6].modules[1].modules[0].modules[0].modules[4].bias\n",
    "    conv4_2_bn_2_mean  = o.modules[6].modules[1].modules[0].modules[0].modules[4].running_mean\n",
    "    conv4_2_bn_2_var   = o.modules[6].modules[1].modules[0].modules[0].modules[4].running_var\n",
    "\n",
    "    conv5_1_weights_skip = o.modules[7].modules[0].modules[0].modules[1].weight\n",
    "    conv5_1_weights_1  = o.modules[7].modules[0].modules[0].modules[0].modules[0].weight\n",
    "    conv5_1_bn_1_gamma = o.modules[7].modules[0].modules[0].modules[0].modules[1].weight\n",
    "    conv5_1_bn_1_beta  = o.modules[7].modules[0].modules[0].modules[0].modules[1].bias\n",
    "    conv5_1_bn_1_mean  = o.modules[7].modules[0].modules[0].modules[0].modules[1].running_mean\n",
    "    conv5_1_bn_1_var   = o.modules[7].modules[0].modules[0].modules[0].modules[1].running_var\n",
    "    conv5_1_weights_2  = o.modules[7].modules[0].modules[0].modules[0].modules[3].weight\n",
    "    conv5_1_bn_2_gamma = o.modules[7].modules[0].modules[0].modules[0].modules[4].weight\n",
    "    conv5_1_bn_2_beta  = o.modules[7].modules[0].modules[0].modules[0].modules[4].bias\n",
    "    conv5_1_bn_2_mean  = o.modules[7].modules[0].modules[0].modules[0].modules[4].running_mean\n",
    "    conv5_1_bn_2_var   = o.modules[7].modules[0].modules[0].modules[0].modules[4].running_var\n",
    "    conv5_2_weights_1  = o.modules[7].modules[1].modules[0].modules[0].modules[0].weight\n",
    "    conv5_2_bn_1_gamma = o.modules[7].modules[1].modules[0].modules[0].modules[1].weight\n",
    "    conv5_2_bn_1_beta  = o.modules[7].modules[1].modules[0].modules[0].modules[1].bias\n",
    "    conv5_2_bn_1_mean  = o.modules[7].modules[1].modules[0].modules[0].modules[1].running_mean\n",
    "    conv5_2_bn_1_var   = o.modules[7].modules[1].modules[0].modules[0].modules[1].running_var\n",
    "    conv5_2_weights_2  = o.modules[7].modules[1].modules[0].modules[0].modules[3].weight\n",
    "    conv5_2_bn_2_gamma = o.modules[7].modules[1].modules[0].modules[0].modules[4].weight\n",
    "    conv5_2_bn_2_beta  = o.modules[7].modules[1].modules[0].modules[0].modules[4].bias\n",
    "    conv5_2_bn_2_mean  = o.modules[7].modules[1].modules[0].modules[0].modules[4].running_mean\n",
    "    conv5_2_bn_2_var   = o.modules[7].modules[1].modules[0].modules[0].modules[4].running_var\n",
    "\n",
    "    fc_weights = o.modules[10].weight\n",
    "    fc_biases = o.modules[10].bias\n",
    "\n",
    "    model_weights_map = {\n",
    "        'initial_block/conv1/kernel': conv1_weights,\n",
    "        'initial_block/bn1/moving_mean': conv1_bn_mean,\n",
    "        'initial_block/bn1/moving_variance': conv1_bn_var,\n",
    "        'initial_block/bn1/beta': conv1_bn_beta,\n",
    "        'initial_block/bn1/gamma': conv1_bn_gamma,\n",
    "\n",
    "        'eb1/bb1/conv_bn_relu/conv/kernel': conv2_1_weights_1,\n",
    "        'eb1/bb1/conv_bn_relu/bn/moving_mean': conv2_1_bn_1_mean,\n",
    "        'eb1/bb1/conv_bn_relu/bn/moving_variance': conv2_1_bn_1_var,\n",
    "        'eb1/bb1/conv_bn_relu/bn/beta': conv2_1_bn_1_beta,\n",
    "        'eb1/bb1/conv_bn_relu/bn/gamma': conv2_1_bn_1_gamma,\n",
    "        'eb1/bb1/conv2/kernel': conv2_1_weights_2,\n",
    "        'eb1/bb1/bn2/moving_mean': conv2_1_bn_2_mean,\n",
    "        'eb1/bb1/bn2/moving_variance': conv2_1_bn_2_var,\n",
    "        'eb1/bb1/bn2/beta': conv2_1_bn_2_beta,\n",
    "        'eb1/bb1/bn2/gamma': conv2_1_bn_2_gamma,\n",
    "        'eb1/bb2/conv_bn_relu/conv/kernel': conv2_2_weights_1,\n",
    "        'eb1/bb2/conv_bn_relu/bn/moving_mean': conv2_2_bn_1_mean,\n",
    "        'eb1/bb2/conv_bn_relu/bn/moving_variance': conv2_2_bn_1_var,\n",
    "        'eb1/bb2/conv_bn_relu/bn/beta': conv2_2_bn_1_beta,\n",
    "        'eb1/bb2/conv_bn_relu/bn/gamma': conv2_2_bn_1_gamma,\n",
    "        'eb1/bb2/conv2/kernel': conv2_2_weights_2,\n",
    "        'eb1/bb2/bn2/moving_mean': conv2_2_bn_2_mean,\n",
    "        'eb1/bb2/bn2/moving_variance': conv2_2_bn_2_var,\n",
    "        'eb1/bb2/bn2/beta': conv2_2_bn_2_beta,\n",
    "        'eb1/bb2/bn2/gamma': conv2_2_bn_2_gamma,\n",
    "\n",
    "        'eb2/bb1/dimension_reduction/kernel': conv3_1_weights_skip,\n",
    "        'eb2/bb1/conv_bn_relu/conv/kernel': conv3_1_weights_1,\n",
    "        'eb2/bb1/conv_bn_relu/bn/moving_mean': conv3_1_bn_1_mean,\n",
    "        'eb2/bb1/conv_bn_relu/bn/moving_variance': conv3_1_bn_1_var,\n",
    "        'eb2/bb1/conv_bn_relu/bn/beta': conv3_1_bn_1_beta,\n",
    "        'eb2/bb1/conv_bn_relu/bn/gamma': conv3_1_bn_1_gamma,\n",
    "        'eb2/bb1/conv2/kernel': conv3_1_weights_2,\n",
    "        'eb2/bb1/bn2/moving_mean': conv3_1_bn_2_mean,\n",
    "        'eb2/bb1/bn2/moving_variance': conv3_1_bn_2_var,\n",
    "        'eb2/bb1/bn2/beta': conv3_1_bn_2_beta,\n",
    "        'eb2/bb1/bn2/gamma': conv3_1_bn_2_gamma,\n",
    "        'eb2/bb2/conv_bn_relu/conv/kernel': conv3_2_weights_1,\n",
    "        'eb2/bb2/conv_bn_relu/bn/moving_mean': conv3_2_bn_1_mean,\n",
    "        'eb2/bb2/conv_bn_relu/bn/moving_variance': conv3_2_bn_1_var,\n",
    "        'eb2/bb2/conv_bn_relu/bn/beta': conv3_2_bn_1_beta,\n",
    "        'eb2/bb2/conv_bn_relu/bn/gamma': conv3_2_bn_1_gamma,\n",
    "        'eb2/bb2/conv2/kernel': conv3_2_weights_2,\n",
    "        'eb2/bb2/bn2/moving_mean': conv3_2_bn_2_mean,\n",
    "        'eb2/bb2/bn2/moving_variance': conv3_2_bn_2_var,\n",
    "        'eb2/bb2/bn2/beta': conv3_2_bn_2_beta,\n",
    "        'eb2/bb2/bn2/gamma': conv3_2_bn_2_gamma,\n",
    "\n",
    "        'eb3/bb1/dimension_reduction/kernel': conv4_1_weights_skip,\n",
    "        'eb3/bb1/conv_bn_relu/conv/kernel': conv4_1_weights_1,\n",
    "        'eb3/bb1/conv_bn_relu/bn/moving_mean': conv4_1_bn_1_mean,\n",
    "        'eb3/bb1/conv_bn_relu/bn/moving_variance': conv4_1_bn_1_var,\n",
    "        'eb3/bb1/conv_bn_relu/bn/beta': conv4_1_bn_1_beta,\n",
    "        'eb3/bb1/conv_bn_relu/bn/gamma': conv4_1_bn_1_gamma,\n",
    "        'eb3/bb1/conv2/kernel': conv4_1_weights_2,\n",
    "        'eb3/bb1/bn2/moving_mean': conv4_1_bn_2_mean,\n",
    "        'eb3/bb1/bn2/moving_variance': conv4_1_bn_2_var,\n",
    "        'eb3/bb1/bn2/beta': conv4_1_bn_2_beta,\n",
    "        'eb3/bb1/bn2/gamma': conv4_1_bn_2_gamma,\n",
    "        'eb3/bb2/conv_bn_relu/conv/kernel': conv4_2_weights_1,\n",
    "        'eb3/bb2/conv_bn_relu/bn/moving_mean': conv4_2_bn_1_mean,\n",
    "        'eb3/bb2/conv_bn_relu/bn/moving_variance': conv4_2_bn_1_var,\n",
    "        'eb3/bb2/conv_bn_relu/bn/beta': conv4_2_bn_1_beta,\n",
    "        'eb3/bb2/conv_bn_relu/bn/gamma': conv4_2_bn_1_gamma,\n",
    "        'eb3/bb2/conv2/kernel': conv4_2_weights_2,\n",
    "        'eb3/bb2/bn2/moving_mean': conv4_2_bn_2_mean,\n",
    "        'eb3/bb2/bn2/moving_variance': conv4_2_bn_2_var,\n",
    "        'eb3/bb2/bn2/beta': conv4_2_bn_2_beta,\n",
    "        'eb3/bb2/bn2/gamma': conv4_2_bn_2_gamma,\n",
    "\n",
    "        'eb4/bb1/dimension_reduction/kernel': conv5_1_weights_skip,\n",
    "        'eb4/bb1/conv_bn_relu/conv/kernel': conv5_1_weights_1,\n",
    "        'eb4/bb1/conv_bn_relu/bn': conv5_1_bn_1_mean,\n",
    "        'eb4/bb1/conv_bn_relu/bn/moving_variance': conv5_1_bn_1_var,\n",
    "        'eb4/bb1/conv_bn_relu/bn/beta': conv5_1_bn_1_beta,\n",
    "        'eb4/bb1/conv_bn_relu/bn/gamma': conv5_1_bn_1_gamma,\n",
    "        'eb4/bb1/conv2/kernel': conv5_1_weights_2,\n",
    "        'eb4/bb1/bn2/moving_mean': conv5_1_bn_2_mean,\n",
    "        'eb4/bb1/bn2/moving_variance': conv5_1_bn_2_var,\n",
    "        'eb4/bb1/bn2/beta': conv5_1_bn_2_beta,\n",
    "        'eb4/bb1/bn2/gamma': conv5_1_bn_2_gamma,\n",
    "        'eb4/bb2/conv_bn_relu/conv/kernel': conv5_2_weights_1,\n",
    "        'eb4/bb2/conv_bn_relu/bn/moving_mean': conv5_2_bn_1_mean,\n",
    "        'eb4/bb2/conv_bn_relu/bn/moving_variance': conv5_2_bn_1_var,\n",
    "        'eb4/bb2/conv_bn_relu/bn/beta': conv5_2_bn_1_beta,\n",
    "        'eb4/bb2/conv_bn_relu/bn/gamma': conv5_2_bn_1_gamma,\n",
    "        'eb4/bb2/conv2/kernel': conv5_2_weights_2,\n",
    "        'eb4/bb2/bn2/moving_mean': conv5_2_bn_2_mean,\n",
    "        'eb4/bb2/bn2/moving_variance': conv5_2_bn_2_var,\n",
    "        'eb4/bb2/bn2/beta': conv5_2_bn_2_beta,\n",
    "        'eb4/bb2/bn2/gamma': conv5_2_bn_2_gamma,\n",
    "\n",
    "    #    'logits/fc/weights': fc_weights,\n",
    "    #    'logits/fc/biases': fc_biases,\n",
    "    }\n",
    "\n",
    "    # Transpose conv and fc weights\n",
    "    model_weights = {}\n",
    "    for k, v in model_weights_map.items():\n",
    "        if len(v.shape) == 4:\n",
    "            model_weights[k] = np.transpose(v, (2, 3, 1, 0))\n",
    "        elif len(v.shape) == 2:\n",
    "            model_weights[k] = np.transpose(v)\n",
    "        else:\n",
    "            model_weights[k] = v\n",
    "\n",
    "    with tf.variable_scope('linknet', reuse=True):\n",
    "        for k, v in model_weights_map.items():\n",
    "            sess.run(tf.get_variable(k).assign(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf1# import tensorflow.compat.v1 as tf1\n",
    "#import tensorflow.contrib.slim as slim\n",
    "import tf_slim as slim\n",
    "from tensorflow.python.keras.initializers import he_normal\n",
    "from tf_slim import arg_scope, add_arg_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee20a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_arg_scope\n",
    "def conv_bn_relu(x, num_channel, kernel_size, stride,\n",
    "                 is_training, scope, padding='same', use_bias=False):\n",
    "    with tf1.compat.v1.variable_scope(scope):\n",
    "        x = keras.layers.Conv2D(num_channel, [kernel_size, kernel_size],\n",
    "                             strides=stride, activation=None, #name='conv',\n",
    "                             padding=padding, use_bias=use_bias,\n",
    "                             kernel_initializer=he_normal())(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9,)(x)\n",
    "                                          #name='bn')\n",
    "        x = tf1.nn.relu(x, name='relu')\n",
    "        return x\n",
    "@add_arg_scope\n",
    "def basic_block(x, num_channel, kernel_size,\n",
    "                stride, is_training, scope, padding='same'):\n",
    "    # Shortcut connection\n",
    "    in_channel = x.get_shape().as_list()[-1]# 64 64 64\n",
    "    \n",
    "    with tf1.compat.v1.variable_scope(scope):\n",
    "        if in_channel == num_channel:\n",
    "            if stride == 1:\n",
    "                shortcut = tf1.identity(x)\n",
    "            else:\n",
    "                shortcut = tf1.nn.max_pool(x, [stride, stride], stride,\n",
    "                                          padding='same')\n",
    "        else:\n",
    "            # Considering maxpooling if stride > 1\n",
    "            shortcut = keras.layers.Conv2D(num_channel, 1, strides=stride,\n",
    "                                        padding='same', activation=None,\n",
    "                                        # name='dimension_reduction',\n",
    "                                        use_bias=False,\n",
    "                                        kernel_initializer=he_normal())(x)\n",
    "            # shortcut = tf1.layers.batch_normalization(\n",
    "            #     shortcut, momentum=0.9, training=is_training,\n",
    "            #     name='shortcut_bn'\n",
    "            # )\n",
    "\n",
    "        x = conv_bn_relu(x, num_channel, kernel_size, stride,\n",
    "                         is_training=is_training, scope='conv_bn_relu',\n",
    "                         padding=padding)\n",
    "        x = keras.layers.Conv2D(num_channel, [kernel_size, kernel_size],\n",
    "                             strides=1, padding=padding, #name='conv2',\n",
    "                             use_bias=False, kernel_initializer=he_normal())(x)\n",
    "        x = keras.layers.BatchNormalization( momentum=0.9,\n",
    "                                           )(x)#name='bn2')# is_trainging delete\n",
    "        # Considering add relu to x before addition\n",
    "        x = x + shortcut\n",
    "        x = tf1.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "@add_arg_scope\n",
    "def encoder_block(x, num_channel, kernel_size, stride, is_training, scope,\n",
    "                 padding='same'):\n",
    "    with tf1.compat.v1.variable_scope(scope):\n",
    "        x = basic_block(x, num_channel, kernel_size, stride, is_training,\n",
    "                        scope=scope+'bb1', padding=padding)# (256,256,64) scope='bb1'\n",
    "        \n",
    "        x = basic_block(x, num_channel, kernel_size, 1, is_training,\n",
    "                        scope=scope+'bb2', padding=padding)\n",
    "       \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "@add_arg_scope\n",
    "def upconv_bn_relu(x, num_channel, kernel_size, stride, is_training,\n",
    "                   scope, padding='same', use_bias=False):\n",
    "    with tf1.compat.v1.variable_scope(scope):\n",
    "        x = keras.layers.Conv2DTranspose( num_channel, [kernel_size, kernel_size],\n",
    "                                      stride, activation=None,\n",
    "                                       padding=padding,# name='conv_transpose',\n",
    "                                      use_bias=False, kernel_initializer=he_normal())(x)\n",
    "        x = keras.layers.BatchNormalization(\n",
    "            momentum=0.9, #training=is_training, #name='bn'\n",
    "        )(x)\n",
    "        x = tf1.nn.relu(x, name='relu')\n",
    "    return x\n",
    "\n",
    "\n",
    "@add_arg_scope\n",
    "def decoder_block(x, num_channel_m, num_channel_n, kernel_size,\n",
    "                  stride=1, is_training=True, scope=None, padding='same'):\n",
    "    with tf1.compat.v1.variable_scope(scope):\n",
    "        x = upconv_bn_relu(x, num_channel_m // 4, 1, 1,\n",
    "                           is_training=is_training,\n",
    "                           scope='conv_transpose_bn_relu1',\n",
    "                           padding=padding)\n",
    "        x = upconv_bn_relu(x, num_channel_m // 4, kernel_size, stride,\n",
    "                           is_training=is_training,\n",
    "                           scope='conv_transpose_bn_relu2',\n",
    "                           padding=padding)\n",
    "        x = upconv_bn_relu(x, num_channel_n, 1, 1,\n",
    "                           is_training=is_training,\n",
    "                           scope='conv_transpose_bn_relu3',\n",
    "                           padding=padding)\n",
    "#         print(\"new___final\",x.shape)\n",
    "#         new___final (None, 64, 64, 256)\n",
    "#         new___final (None, 128, 128, 128)\n",
    "#         new___final (None, 256, 256, 64)\n",
    "#         new___final (None, 256, 256, 64)\n",
    "    return x\n",
    "\n",
    "\n",
    "@add_arg_scope\n",
    "def initial_block(x, scope='initial_block',\n",
    "                  padding='same', use_bias=False):\n",
    "    with tf1.compat.v1.variable_scope(scope):\n",
    "        x = keras.layers.Conv2D(64, 7, strides=(2,2), activation=None,\n",
    "                             name='conv1', padding=padding, use_bias=use_bias,\n",
    "                             kernel_initializer=he_normal())(x)\n",
    "#         print(\"new____new\",x.shape)# (512,512,64)\n",
    "        x = keras.layers.BatchNormalization(\n",
    "            momentum=0.9, name='bn1'\n",
    "        )(x)\n",
    "        x = keras.layers.ReLU(name='relu')(x)\n",
    "        x = keras.layers.MaxPooling2D(pool_size=(3,3),strides=2,name='maxpool',padding=padding)(x) \n",
    "          \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linknet(num_classes=NUM_CLASSES, width = WIDTH, height = HEIGHT, reuse=None, is_training=True,\n",
    "            scope='linknet'):\n",
    "\n",
    "    inputs = keras.Input(shape=(width, height, 3), name=\"Image\")\n",
    "\n",
    "    filters = [64, 128, 256, 512]\n",
    "    filters_m = [64, 128, 256, 512][::-1]\n",
    "    filters_n = [64, 64, 128, 256][::-1]\n",
    "\n",
    "    with tf1.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "            # Encoder\n",
    "            eb0 = initial_block(inputs, \n",
    "                                scope='initial_block')\n",
    "#             print(\"eb0.shape\",eb0.shape)#eb0.shape (None, 256, 256, 64)\n",
    "            eb1 = encoder_block(eb0, filters[0], 3, 1, is_training,\n",
    "                                scope='eb1', padding='same')# (256,256,64)\n",
    "            ebi = eb1\n",
    "            ebs = [eb1, ]\n",
    "            i = 2\n",
    "            for filter_i in filters[1:]:\n",
    "                \n",
    "                ebi = encoder_block(ebi, filter_i, 3, 2, is_training,\n",
    "                                scope='eb'+str(i), padding='same')\n",
    "                \n",
    "                ebs.append(ebi)\n",
    "                i = i + 1\n",
    "            net = ebi# (32,32,512)\n",
    "\n",
    "            # Decoder\n",
    "            dbi = decoder_block(net, filters_m[0], filters_n[0], 3,\n",
    "                                2, is_training=is_training, scope='db4',\n",
    "                                padding='same')\n",
    "            i = len(filters_m) - 1\n",
    "            for filters_i in zip(filters_m[1:-1], filters_n[1:-1]):\n",
    "                dbi = dbi + ebs[i-1]\n",
    "                dbi = decoder_block(dbi, filters_i[0], filters_i[1], 3,\n",
    "                                    2, is_training=is_training,\n",
    "                                    scope='db'+str(i), padding='same')\n",
    "                i = i - 1\n",
    "            dbi = dbi + ebs[0]\n",
    "            dbi = decoder_block(dbi, filters_m[-1], filters_n[-1], 3, 1,\n",
    "                                is_training=is_training,\n",
    "                                scope='db1', padding='same')\n",
    "            net = dbi\n",
    "\n",
    "            # Classification\n",
    "            with tf1.compat.v1.variable_scope('classifier', reuse=reuse):\n",
    "                net = upconv_bn_relu(net, 32, 3, 2, is_training=is_training,\n",
    "                                     scope='conv_transpose')\n",
    "                net = conv_bn_relu(net, 32, 3, 1, is_training=is_training,\n",
    "                                   scope='conv')\n",
    "                # Last layer, no batch normalization or activation\n",
    "                logits = keras.layers.Conv2DTranspose(num_classes,\n",
    "                                                    kernel_size=2, strides=2,\n",
    "                                                    padding='same',\n",
    "                                                    name='conv_transpose',\n",
    "                                                    kernel_initializer=he_normal())(net)\n",
    "#                 print(\"logits.shape\",logits.shape) # logits.shape (None, 1024, 1024, 3)\n",
    "                if num_classes > 1:\n",
    "                    prob = tf1.nn.softmax(logits, name='prob')\n",
    "                else:\n",
    "                    prob = tf1.nn.sigmoid(logits, name='prob')\n",
    "                    \n",
    "               \n",
    "\n",
    "                model = tf1.keras.models.Model(inputs=inputs,outputs=prob)\n",
    "                return model\n",
    "                \n",
    "#                 return prob, logits         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b0c48",
   "metadata": {},
   "source": [
    "# Loss Function and Generator Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a1f5e",
   "metadata": {},
   "source": [
    "dice loss, IDRiD-S+DiaretDB1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ea3c0",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f59f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linknet()\n",
    "vgg_CFL = MODEL()\n",
    "vgg_CFL.Run_training()\n",
    "vgg_CFL.Evaluation(num_sample = -1)\n",
    "del vgg_CFL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
